{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = \"merged_support3\"\n",
    "target_feature_suffix = \"_matrix.npy\"\n",
    "save_data_path = \"./results\"\n",
    "\n",
    "pre_selection_methods = [\"variance\"] #\"chi2\", \"f_classif\"\n",
    "n_pre_select_list = [1000000] #[2000000, 4000000, 8000000, 16000000, 32000000]#\n",
    "n_pre_select_goal = 1000000\n",
    "\n",
    "select_methods = [\"xgb\"]# [\"random\", \"xgb\", \"rf\", \"variance\", \"chi2\", \"f_classif\"] # Extra-trees # \"mutual_info_classif\"\n",
    "\n",
    "select_feature_from_cache = False\n",
    "n_select_list = [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072] \n",
    "\n",
    "n_dim_reduce_list = [128, 256, 512, 1024, None]  ## list should always contain None to perform whole feature training after selection\n",
    "ML_models = [\"SVM\"] #[\"SVM\", \"XGB\", \"RF\", \"DT\", \"KNN\"]\n",
    "\n",
    "hyper_params = {\n",
    "    \"SVM\": [\n",
    "            {'C': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'kernel': ['linear']}  # grid search params\n",
    "            ],\n",
    "    \"RF\": [\n",
    "        {'n_estimators': 500, 'max_features': 'sqrt', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}, # current best for 1048576 features\n",
    "    ],\n",
    "\n",
    "    \"XGB\": [\n",
    "        {'learning_rate': 0.1, 'n_estimators': 1000, 'max_depth': 3, 'gamma': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_lambda': 1, 'reg_alpha': 0}, # current best for 1048576 features\n",
    "    ]\n",
    "}\n",
    "save_result_file_name = f\"{target_feature}_results.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_path, sample_annotation_file = get_data_path(save_data_path)\n",
    "\n",
    "dataset = data_loader(os.path.join(feature_data_path, target_feature + target_feature_suffix), \n",
    "                        sample_annotation_file)\n",
    "(X, y_original, y), (train_indices, val_indices, test_indices), label_mapping = dataset.get_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preliminary feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_combined = []\n",
    "for pre_feature_select_method in pre_selection_methods:\n",
    "    try:\n",
    "        X_pre_selected_list, perf_metric_preselect = select_feature(X = X, y = y, method = pre_feature_select_method, n_list = n_pre_select_list, train_idx = train_indices, val_idx = val_indices) \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while pre_select_feature of {pre_feature_select_method}. {e.__class__.__name__}: {str(e)}\")\n",
    "        continue \n",
    "\n",
    "    for X_pre_selected, n_pre_select in zip(X_pre_selected_list, n_pre_select_list):\n",
    "        if n_pre_select > n_pre_select_goal:\n",
    "            try:\n",
    "                X_pre_selected_final_list, _ = select_feature(X = X_pre_selected, y = y, method = \"random\", n_list = [n_pre_select_goal], train_idx = train_indices, val_idx = val_indices) \n",
    "            except Exception as e:\n",
    "                logging.error(f\"An unexpected error occurred while random selection after pre_select_feature. {e.__class__.__name__}: {str(e)}\")\n",
    "                continue \n",
    "            X_pre_selected_final = X_pre_selected_final_list[0]\n",
    "            logging.info(f\" - Further selecting feature by random from {n_pre_select} to {n_pre_select_goal} variants. X_pre_selected.shape = {X_pre_selected.shape}. X_pre_selected_final.shape = {X_pre_selected_final.shape}\")\n",
    "        else:\n",
    "            X_pre_selected_final = X_pre_selected\n",
    "        logging.info(f\" - '{pre_feature_select_method}' feature selection selected {min(n_pre_select, n_pre_select_goal)} variants. X_pre_selected_final.shape = {X_pre_selected_final.shape}. perf_metrics_selection: {perf_metric_preselect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# secondary feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_select_method in select_methods:\n",
    "    current_loop = {\"random_seed\": RANDOM_SEED, \"pre_select_method\": pre_feature_select_method, \"n_pre_select\": n_pre_select, \"n_pre_select_goal\": n_pre_select_goal, \"select_method\": feature_select_method}\n",
    "    feature_importance_cache_file_prefix = f\"{X.shape[1]}_seed{RANDOM_SEED}_{pre_feature_select_method}_{n_pre_select}_{n_pre_select_goal}_{feature_select_method}\"\n",
    "\n",
    "    logging.info(f\"*************** current loop: {current_loop} ***************\")\n",
    "\n",
    "    try:\n",
    "        X_selected_list, perf_metric_select = select_feature(X = X_pre_selected_final, y = y, method = feature_select_method, n_list = n_select_list, train_idx = train_indices, val_idx = val_indices, cache_file_prefix = feature_importance_cache_file_prefix, from_cache = select_feature_from_cache) \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while select_feature of {current_loop}. {e.__class__.__name__}: {str(e)}\")\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_selected, n_select in zip(X_selected_list, n_select_list):\n",
    "    current_loop[\"select_n\"] = n_select\n",
    "\n",
    "    logging.info(f\" - '{feature_select_method}' feature selection selected {n_select} variants. X_selected.shape = {X_selected.shape}. perf_metrics_selection: {perf_metric_select}\")\n",
    "\n",
    "    if len(X_selected.shape) == 3: # boolean encoding of SNP status\n",
    "        X_selected = X_selected.reshape(X_selected.shape[0], -1) #flatten last feature dims\n",
    "    X_train, X_val, X_test = X_selected[train_indices], X_selected[val_indices], X_selected[test_indices]\n",
    "    y_train, y_val, y_test = y[train_indices], y[val_indices], y[test_indices]\n",
    "    \n",
    "    for n_dim_reduced in n_dim_reduce_list:\n",
    "        if (n_dim_reduced is None): # use whole feature\n",
    "            current_loop[\"n_dim_reduced\"] = n_select\n",
    "            X_train_reduced, X_val_reduced, X_test_reduced = X_train, X_val, X_test \n",
    "            logging.info(f\" - Using whole features for training: X_train.shape = {X_train_reduced.shape}\")\n",
    "        else:\n",
    "            if (n_dim_reduced < n_select):\n",
    "                current_loop[\"n_dim_reduced\"] = n_dim_reduced\n",
    "                try:\n",
    "                    X_train_reduced, X_val_reduced, X_test_reduced = feature_transform(X_train, X_val, X_test, n = n_dim_reduced)\n",
    "                    logging.info(f\" - Reduced to {n_dim_reduced} features using PCA: X_train_reduced.shape = {X_train_reduced.shape}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"An unexpected error occurred while feature_transform of {current_loop}. {e.__class__.__name__}: {str(e)}\")\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    \n",
    "        for train_model in ML_models:\n",
    "            for hyper_param_index, current_hyper_param in enumerate(hyper_params[train_model]):\n",
    "                current_loop[\"train_model\"] = train_model\n",
    "\n",
    "                logging.info(f\" - Start {train_model} training: X_train.shape = {X_train_reduced.shape} X_test.shape = {X_test_reduced.shape} with hyper_param {current_hyper_param}\")\n",
    "\n",
    "                try:\n",
    "                    (y_pred_train, y_pred_val, y_pred_test, train_params), perf_metric_train = train_ML(method = train_model, \n",
    "                                                                        X_train = X_train_reduced, y_train = y_train, \n",
    "                                                                        X_val = X_val_reduced, y_val = y_val, \n",
    "                                                                        X_test = X_test_reduced,\n",
    "                                                                        params = current_hyper_param) \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"An unexpected error occurred while train_ML of {current_loop}. {e.__class__.__name__}: {str(e)}\")\n",
    "                    continue \n",
    "                eval_metrics_train = evaluate_performance(y_train, y_pred_train, label_mapping, os.path.join(save_data_path, f\"{feature_select_method}_{n_select}_{train_model}_{hyper_param_index}_train\"))\n",
    "                eval_metrics_val = evaluate_performance(y_val, y_pred_val, label_mapping, os.path.join(save_data_path, f\"{feature_select_method}_{n_select}_{train_model}_{hyper_param_index}_val\"))\n",
    "                eval_metrics_test = evaluate_performance(y_test, y_pred_test, label_mapping, os.path.join(save_data_path, f\"{feature_select_method}_{n_select}_{train_model}_{hyper_param_index}_test\"))\n",
    "                logging.info(f' - Train done with Accuracy: {eval_metrics_test[\"accuracy\"]*100:.4f}%, perf_metrics_train: {perf_metric_train}')\n",
    "\n",
    "\n",
    "                merged_metrics = {**current_loop,\n",
    "                                \"hyper_params\" : str(current_hyper_param),\n",
    "                                \"model_params\" : str(train_params),\n",
    "                                **{f\"preselect_{k}\": v for k, v in perf_metric_preselect.items()},\n",
    "                                **{f\"select_{k}\": v for k, v in perf_metric_select.items()},\n",
    "                                **{f\"train_{k}\": v for k, v in perf_metric_train.items()},\n",
    "                                **{f\"testset_{k}\": v for k, v in eval_metrics_test.items() if k != 'confusion_matrix'},\n",
    "                                **{f\"valset_{k}\": v for k, v in eval_metrics_val.items() if k != 'confusion_matrix'},\n",
    "                                **{f\"trainset_{k}\": v for k, v in eval_metrics_train.items() if k != 'confusion_matrix'},\n",
    "                                }\n",
    "                result_combined.append(merged_metrics)\n",
    "\n",
    "                ## update the dataframe\n",
    "                results_df = pd.DataFrame(result_combined)\n",
    "                results_df.to_excel(os.path.join(save_data_path, save_result_file_name), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preliminary_selection_file = \"43034818_seed42_variance.npy\"\n",
    "secondary_selection_file = \"43034818_seed42_variance_1000000_1000000_xgb_basic_feature_importance_mean.npy\"\n",
    "n_preliminary = n_pre_select_list[0]\n",
    "n_secondary = 8192\n",
    "\n",
    "output_file_name = \"merged_support3_variance_1M_seed_42_xgb_8192_matrix.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading features from file /home/jinhyun/data/1kGP/preprocessed/merged_support3_matrix.npy. Data shape : (3202, 43034818)\n"
     ]
    }
   ],
   "source": [
    "feature_data_path, sample_annotation_file = get_data_path(save_data_path)\n",
    "feature_file_name = os.path.join(feature_data_path, target_feature + target_feature_suffix)\n",
    "X = np.load(feature_file_name)\n",
    "print(f\"reading features from file {feature_file_name}. Data shape : {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected variants based on variance filter. Shape: (3202, 1000000)\n"
     ]
    }
   ],
   "source": [
    "variances = np.load(preliminary_selection_file)\n",
    "preliminary_selected_indices = np.argsort(variances)[-n_preliminary:]\n",
    "\n",
    "boolean_mask = np.zeros(X.shape[1], dtype=bool)\n",
    "boolean_mask[preliminary_selected_indices] = True\n",
    "\n",
    "X_selected_preliminary = X[:, boolean_mask]\n",
    "\n",
    "assert boolean_mask.sum() == X_selected_preliminary.shape[1]\n",
    "print(f\"Selected variants based on variance filter. Shape: {X_selected_preliminary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected variants based on XGB. Shape: (3202, 8192)\n"
     ]
    }
   ],
   "source": [
    "secondary_feature_importance = np.load(secondary_selection_file)\n",
    "secondary_selected_indices = np.argsort(secondary_feature_importance)[-n_secondary:][::-1]\n",
    "X_selected_secondary = X_selected_preliminary[:, secondary_selected_indices]\n",
    "assert secondary_feature_importance.shape[0] == X_selected_preliminary.shape[1]\n",
    "print(f\"Selected variants based on XGB. Shape: {X_selected_secondary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the result to file merged_support3_variance_1M_seed_42_xgb_8192_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "np.save(os.path.join(feature_data_path, output_file_name), X_selected_secondary)\n",
    "print(f\"Saved the result to file {output_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val, method):\n",
    "    if method == \"SVM\":\n",
    "        C = trial.suggest_loguniform('C', 1e-6, 10)  # Log-uniform distribution for C\n",
    "        kernel = \"linear\"\n",
    "        gamma = 'scale'\n",
    "        \n",
    "        # kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid'])  # Categorical distribution for kernel type\n",
    "        # if kernel == 'rbf' or kernel == 'sigmoid':\n",
    "        #     gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])  # Categorical for gamma if relevant\n",
    "        # else:\n",
    "        #     gamma = 'scale'  # Default to 'scale' for linear to avoid irrelevance\n",
    "        model = SVC(C=C, kernel=kernel, gamma=gamma, random_state=RANDOM_SEED)\n",
    "        \n",
    "    elif method == \"RF\":\n",
    "        n_estimators = trial.suggest_int('n_estimators', 100, 1000, step=100)\n",
    "        max_depth = trial.suggest_int('max_depth', 10, 100, step=10)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators, max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "            random_state=RANDOM_SEED)\n",
    "    elif method == \"XGB\":\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.1)\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=100)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.1, 1.0, step=0.1)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "        model = XGBClassifier(\n",
    "            learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth,\n",
    "            gamma=gamma, subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "            objective='multi:softmax', num_class=len(np.unique(y_train)),\n",
    "            use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_SEED)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "    trial.set_user_attr(\"train_accuracy\", train_accuracy)\n",
    "    return val_accuracy\n",
    "\n",
    "def visualize_study(study):\n",
    "    optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "    plt.show()\n",
    "\n",
    "    optuna.visualization.matplotlib.plot_parallel_coordinate(study)\n",
    "    plt.show()\n",
    "\n",
    "    # Additional plotting can be customized based on user's needs.\n",
    "    # For instance, plot train vs validation accuracy:\n",
    "    train_accuracies = [trial.user_attrs[\"train_accuracy\"] for trial in study.trials]\n",
    "    val_accuracies = [trial.value for trial in study.trials]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, X_train_reduced, y_train, X_val_reduced, y_val, method=train_model), n_trials=50)\n",
    "\n",
    "best_rf_params = study.best_trial.params\n",
    "print(\"Best RF Params:\", best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_study(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = np.load(\"1048576_seed42_xgb_basic_feature_importance_mean.npy\")\n",
    "fi = fi[np.argsort(fi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "pd.Series(fi[-100:]).plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(fi, bins = 100)\n",
    "plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinhyun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
